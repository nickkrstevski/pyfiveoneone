import os
from typing import cast
import importlib
import pandas as pd
import numpy as np
import calendar
import zipfile


from pyfiveoneone.autogenerated.stops import Stops
from pyfiveoneone.client import Client

data_dir = "./cache/"

client = Client()


def pull_and_filter_raw_historical_data(MM: str, YYYY: str, operator_id: str = "CT"):
    return download_extract_filter_month(YYYY=YYYY, MM=MM, operator_id=operator_id)


def load_ct_historical_data(file_path: str):
    df = pd.read_csv(f"{data_dir}{file_path}", delimiter=",")

    return df


def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _find_file(root_dir: str, filename: str) -> str | None:
    for dirpath, _, filenames in os.walk(root_dir):
        if filename in filenames:
            return os.path.join(dirpath, filename)
    return None


def download_extract_filter_month(
    YYYY: str, MM: str, operator_id: str = "CT"
) -> pd.DataFrame:
    _ensure_dir(data_dir)
    # Paths
    month_tag = f"{YYYY}-{MM}"
    zip_path = os.path.join(data_dir, f"{month_tag}-{operator_id}.zip")
    extract_dir = os.path.join(data_dir, f"{month_tag}-{operator_id}")
    monthly_csv_path = os.path.join(data_dir, f"{month_tag}-{operator_id}.csv")

    # If already processed CSV exists, load and return
    if os.path.exists(monthly_csv_path):
        cached_df: pd.DataFrame = pd.read_csv(monthly_csv_path)
        return cached_df

    # If extracted directory already exists, skip download/extract
    if not os.path.exists(extract_dir):
        # Download ZIP if missing
        if not os.path.exists(zip_path):
            client.gtfs_feed_download(operator_id, zip_path, MM=MM, YYYY=YYYY)

        # Extract ZIP
        _ensure_dir(extract_dir)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(extract_dir)

        # Delete original ZIP
        try:
            os.remove(zip_path)
        except OSError:
            pass

    # Find stop_observations.txt
    stop_obs_path = _find_file(extract_dir, "stop_observations.txt")
    if stop_obs_path is None:
        raise FileNotFoundError(f"stop_observations.txt not found under {extract_dir}")

    # Load, filter, save
    df = pd.read_csv(stop_obs_path, delimiter=",")
    indexer: pd.Series = df["trip_id"].astype(str).str.contains(operator_id, na=False)
    filtered: pd.DataFrame = df.loc[indexer].copy()
    filtered.to_csv(monthly_csv_path, index=False)
    return filtered


def build_year_dataframe(YYYY: str, operator_id: str = "CT") -> pd.DataFrame:
    frames: list[pd.DataFrame] = []
    for m in range(1, 9):
        MM = f"{m:02d}"
        try:
            monthly_df = download_extract_filter_month(
                YYYY=YYYY, MM=MM, operator_id=operator_id
            )
            frames.append(monthly_df)
        except Exception as e:
            print(f"Skipping {YYYY}-{MM} due to error: {e}")
    if not frames:
        return pd.DataFrame()
    big_df = pd.concat(frames, ignore_index=True)
    big_out = os.path.join(data_dir, f"{YYYY}-{operator_id}-ALL.csv")
    big_df.to_csv(big_out, index=False)
    return big_df


def build_year_dataframe_from_cache(YYYY: str, operator_id: str = "CT") -> pd.DataFrame:
    # Use any monthly CSVs already present in cache
    import glob as _glob

    pattern = os.path.join(data_dir, f"{YYYY}-??-{operator_id}.csv")
    csv_files = sorted(_glob.glob(pattern))
    if not csv_files:
        return pd.DataFrame()
    frames: list[pd.DataFrame] = []
    for path in csv_files:
        try:
            frames.append(pd.read_csv(path))
        except Exception as e:
            print(f"Skipping cached file {path}: {e}")
    if not frames:
        return pd.DataFrame()
    big_df = pd.concat(frames, ignore_index=True)
    # Write/update combined cache
    big_out = os.path.join(data_dir, f"{YYYY}-{operator_id}-ALL.csv")
    try:
        big_df.to_csv(big_out, index=False)
    except Exception:
        pass
    return big_df


def _parse_datetime(df: pd.DataFrame, date_col: str, time_col: str) -> pd.Series:
    time_series: pd.Series = cast(pd.Series, df[time_col])
    # Prefer combining service_date + time when service_date is available
    if date_col in df.columns:
        service_series: pd.Series = cast(pd.Series, df[date_col])
        service_dt = pd.to_datetime(service_series, errors="coerce")
        if service_dt.notna().any():
            # Build timedelta from time column
            if (
                pd.api.types.is_string_dtype(time_series)
                or time_series.astype(str).str.contains(":").any()
            ):
                time_td = pd.to_timedelta(time_series, errors="coerce")
            elif pd.api.types.is_numeric_dtype(time_series):
                time_td = pd.to_timedelta(time_series, unit="s", errors="coerce")
            else:
                time_td = pd.to_timedelta(time_series, errors="coerce")
            return service_dt + time_td

    # Fallback: attempt to parse as full datetimes or epochs
    dt = pd.to_datetime(time_series, errors="coerce")
    if dt.notna().mean() < 0.5 and pd.api.types.is_numeric_dtype(time_series):
        vals = cast(pd.Series, time_series)
        non_null = vals.dropna()
        median_val: float = float(non_null.median()) if not non_null.empty else 0.0
        unit = "ms" if median_val > 1e12 else "s"
        dt = pd.to_datetime(vals, unit=unit, errors="coerce")
    return dt


def plot_sf_nb_arrival_delay_histogram(source_df: pd.DataFrame) -> str:
    # Resolve stop_id for 22nd Street Northbound
    stop_22nd_nb_id = None
    if Stops is not None:
        try:
            stop_22nd_nb_id = str(
                Stops.STOP_22ND_STREET_CALTRAIN_STATION_NORTHBOUND.value.stop_id
            )
        except Exception:
            stop_22nd_nb_id = "70021"
    else:
        stop_22nd_nb_id = "70021"

    # Filter for arrivals at 22nd Street Northbound (match on to_stop_id)
    to_stop_series: pd.Series = cast(pd.Series, source_df["to_stop_id"]).astype(str)
    to_stop_numeric: pd.Series = cast(
        pd.Series, pd.to_numeric(to_stop_series, errors="coerce")
    ).astype("Int64")

    try:
        stop_22nd_nb_id_num = int(stop_22nd_nb_id)
    except Exception:
        stop_22nd_nb_id_num = None

    # Get arrivals at 22nd Street Northbound
    if stop_22nd_nb_id_num is not None:
        arrivals_22nd = cast(
            pd.DataFrame,
            source_df.loc[to_stop_numeric == stop_22nd_nb_id_num].copy(),
        )
    else:
        arrivals_22nd = cast(
            pd.DataFrame,
            source_df[source_df["to_stop_id"].astype(str) == stop_22nd_nb_id].copy(),
        )

    sf_nb = arrivals_22nd

    # Build datetimes and compute delay (observed - scheduled) in minutes
    # Use arrival times since we're looking at arrivals at 22nd Street
    scheduled_dt = _parse_datetime(sf_nb, "service_date", "scheduled_arrival_time")
    observed_dt = _parse_datetime(sf_nb, "service_date", "observed_arrival_time")
    sf_nb["arrival_delay_min"] = (observed_dt - scheduled_dt).dt.total_seconds() / 60.0
    sf_nb = sf_nb.dropna(subset=("arrival_delay_min",))  # keep valid rows only
    # Derive month from service_date directly (YYYYMMDD)
    sf_nb["month"] = pd.to_datetime(
        sf_nb["service_date"].astype(str), format="%Y%m%d", errors="coerce"
    ).dt.month

    # Plot histogram (lazy import to avoid hard dependency during linting)
    output_path = os.path.join(data_dir, "2025-22nd-nb-arrival-delay-hist.png")
    try:
        plt = importlib.import_module("matplotlib.pyplot")
        plt.figure(figsize=(9, 5))
        # Common bins across all months
        data_all = sf_nb["arrival_delay_min"].to_numpy()
        if data_all.size == 0:
            raise ValueError("No arrival delay data available to plot")
        data_all = data_all[~np.isnan(data_all)]
        # Use full data range but limit display to 60 minutes
        data_min, data_max = data_all.min(), min(data_all.max(), 60.0)
        # Create 1-minute wide bins, properly handling negative values
        bin_start = int(np.floor(data_min))  # Floor for negative values
        bin_end = int(np.ceil(data_max))  # Ceiling for positive values
        bins = np.arange(bin_start, bin_end + 1, 1.0)  # 1-minute bins

        # Diagnostics to validate construction
        try:
            month_counts = sf_nb["month"].value_counts().sort_index().to_dict()
            print({"sf_month_counts": month_counts})
            q = sf_nb["arrival_delay_min"].quantile([0.05, 0.5, 0.95]).to_dict()
            print({"delay_quantiles_min": q})
            # Show data range to verify no cropping
            print({"data_range_min": {"min": float(data_min), "max": float(data_max)}})
        except Exception:
            pass

        # Colors per month using a colormap
        cmap = importlib.import_module("matplotlib.cm").get_cmap("tab20")
        colors = [cmap(i / 12.0) for i in range(12)]

        # Overlay per-month transparent histograms
        handles = []
        labels = []
        for m in range(1, 13):
            month_data = sf_nb.loc[sf_nb["month"] == m, "arrival_delay_min"].to_numpy()
            month_data = month_data[~np.isnan(month_data)]
            if month_data.size == 0:
                continue
            month_label = f"{calendar.month_abbr[m]} (n={month_data.size})"
            h = plt.hist(
                month_data,
                bins=bins,
                alpha=0.25,
                color=colors[m - 1],
                label=month_label,
                edgecolor="none",
            )
            handles.append(h[2])
            labels.append(month_label)

        # 95th percentile vertical line (overall)
        p95 = float(np.nanpercentile(data_all, 95))
        p95_minutes = int(p95)
        p95_seconds = int((p95 - p95_minutes) * 60)
        plt.axvline(
            p95,
            color="#333333",
            linestyle="--",
            linewidth=2,
            label=f"95th %ile ({p95_minutes}m {p95_seconds}s)",
        )

        # Calculate statistics for text box
        mean_delay = float(np.nanmean(data_all))
        median_delay = float(np.nanmedian(data_all))
        std_delay = float(np.nanstd(data_all))

        # 13 minute line and calculate its percentile
        thirteen_min_percentile = float(np.sum(data_all <= 13.0) / len(data_all) * 100)
        plt.axvline(
            13.0,
            color="#FF6B6B",
            linestyle="-",
            linewidth=2,
            label=f"13 min ({thirteen_min_percentile:.1f}%ile)",
        )

        # Add statistics text box in bottom right
        stats_text = f"Mean: {mean_delay:.1f} min\nMedian: {median_delay:.1f} min\nStd Dev: {std_delay:.1f} min"
        plt.text(
            0.98,
            0.02,
            stats_text,
            transform=plt.gca().transAxes,
            fontsize=10,
            verticalalignment="bottom",
            horizontalalignment="right",
            bbox=dict(boxstyle="round", facecolor="white", alpha=0.8, edgecolor="gray"),
        )

        plt.title(
            "Caltrain to San Francisco: Minutes late upon arrival — 2025",
            fontsize=16,
        )
        plt.xlabel("Minutes late (+) / early (-)")
        plt.ylabel("Count")
        plt.xlim(
            bin_start, 60
        )  # Show full range including negative values, cap at 60 minutes

        # Add 1-minute subticks
        import matplotlib.ticker as ticker

        ax = plt.gca()
        ax.xaxis.set_major_locator(
            ticker.MultipleLocator(5)
        )  # Major ticks every 5 minutes
        ax.xaxis.set_minor_locator(
            ticker.MultipleLocator(1)
        )  # Minor ticks every 1 minute

        plt.legend(fontsize=10, ncol=4)
        plt.tight_layout()
        plt.savefig(output_path, bbox_inches="tight")
        plt.close()
    except Exception as e:
        print(f"Skipping plot generation (matplotlib not available): {e}")

    # Optional: basic stats in console
    if not sf_nb["arrival_delay_min"].empty:
        stats = {
            "count": int(sf_nb["arrival_delay_min"].count()),
            "mean_min": float(sf_nb["arrival_delay_min"].mean()),
            "median_min": float(sf_nb["arrival_delay_min"].median()),
            "std_min": float(sf_nb["arrival_delay_min"].std()),
            "p90_min": float(sf_nb["arrival_delay_min"].quantile(0.90)),
        }
        print(stats)

    print(f"Saved histogram to {output_path}")
    return output_path


# Build 2025 dataset using cache first, then download if needed
all_2025_df = build_year_dataframe_from_cache("2025", operator_id="CT")
if all_2025_df.empty:
    all_2025_df = build_year_dataframe("2025", operator_id="CT")
if not all_2025_df.empty:
    plot_sf_nb_arrival_delay_histogram(all_2025_df)
