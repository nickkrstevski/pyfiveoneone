#!/usr/bin/env python3
import csv
import re
import sys
from dataclasses import fields, is_dataclass
from importlib import import_module
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime, timezone

# import hashlib
import textwrap

ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = ROOT / "src"
PKG = "pyfiveoneone"
OUT_DIR = SRC_DIR / PKG / "autogenerated"
INPUT_DIR = OUT_DIR / "inputs" / "GTFSTransitData_CT"
MARKER_BEGIN = "# === BEGIN ENUM BODY ==="
MARKER_END = "# === END ENUM BODY ==="

# Map model name to (CSV path relative to repo root, Enum class name, preferred label columns)
MODEL_SPECS: Dict[str, Tuple[Path, str, Tuple[str, ...]]] = {
    # Model: (csv_path, enum_name, label_columns_for_slug)
    "Direction": (
        INPUT_DIR / "directions.txt",
        "Directions",
        ("direction_name", "direction_id"),
    ),
    "Stop": (
        INPUT_DIR / "stops.txt",
        "Stops",
        ("stop_name", "stop_code", "stop_id"),
    ),
    "Route": (
        INPUT_DIR / "routes.txt",
        "Routes",
        ("route_long_name", "route_short_name", "route_id"),
    ),
    "Trip": (
        INPUT_DIR / "trips.txt",
        "Trips",
        ("trip_headsign", "route_id", "trip_id"),
    ),
    "CalendarAttribute": (
        INPUT_DIR / "calendar_attributes.txt",
        "CalendarAttributes",
        ("service_description", "service_id"),
    ),
}


def slugify(name: str, prefix: str) -> str:
    s = (name or "").strip().upper()
    s = re.sub(r"[^A-Z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    if not s or s[0].isdigit():
        s = f"{prefix}_{s}" if s else prefix
    return s


def import_model(model_name: str):
    sys.path.insert(0, str(SRC_DIR))
    mod = import_module(f"{PKG}.models")
    cls = getattr(mod, model_name)
    if not is_dataclass(cls):
        raise TypeError(f"{model_name} is not a dataclass")
    return cls


def read_csv_rows(csv_path: Path) -> List[dict]:
    with csv_path.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        return rows


def coerce_value(value: Any, annotation: object):
    if value is None:
        return None
    text = str(value)
    origin = annotation
    try:
        if origin is int:
            return int(text) if text else 0
        if origin is float:
            return float(text) if text else 0.0
        # default to string
        return text
    except Exception:
        # Fallback to string on coercion failure
        return text


def build_enum_body(model_name: str, enum_name: str, rows: List[dict]) -> str:
    """Return only the enum class + members (deterministic, sorted)."""
    cls = import_model(model_name)
    dataclass_fields = {f.name: f.type for f in fields(cls)}
    preferred_labels = MODEL_SPECS[model_name][2]

    # Build temporary list of (member_name, kwargs_str)
    used_names = set()
    items: List[Tuple[str, str]] = []

    # Sort input rows deterministically by first available label, then by any id/code
    def label_key(row: dict) -> str:
        label_val = next((row.get(col) for col in preferred_labels if row.get(col)), "")
        return (label_val or "").lower()

    rows_sorted = sorted(
        rows,
        key=lambda r: (
            label_key(r),
            r.get("stop_code") or r.get("stop_id") or r.get("route_id") or "",
        ),
    )

    for row in rows_sorted:
        # Determine enum member name
        label_val = next(
            (row.get(col) for col in preferred_labels if row.get(col)), None
        )
        enum_prefix = model_name.upper()
        member_name = slugify(label_val or "", enum_prefix)
        # Disambiguate if duplicate
        if member_name in used_names:
            id_field = next(
                (k for k in ("stop_code", "stop_id", "route_id") if row.get(k)), None
            )
            if id_field:
                member_name = f"{member_name}_{row[id_field]}".upper()
        used_names.add(member_name)

        # Build kwargs limited to fields present in both dataclass and CSV
        kwargs_items = []
        for field_name, annotation in dataclass_fields.items():
            if field_name in row:
                value = coerce_value(row.get(field_name), annotation)
                kwargs_items.append(f"{field_name}={repr(value)}")
        kwargs_str = ", ".join(kwargs_items)
        items.append((member_name, kwargs_str))

    # Sort by member name for deterministic order
    items.sort(key=lambda x: x[0])

    # Emit class and members
    body_lines: List[str] = []
    body_lines.append(f"class {enum_name}(Enum):\n")
    body_lines.append(
        textwrap.indent(
            f'"""Autocomplete-friendly enumeration of {model_name} rows.\n\nAccess dataclass instances via: {enum_name}.FOO.value\n"""\n',
            "\t",
        )
    )
    for member_name, kwargs_str in items:
        body_lines.append(f"\t{member_name} = {model_name}({kwargs_str})\n")
    return "".join(body_lines)


def split_existing_body(content: str) -> Tuple[str, str, str]:
    """Return (prefix, body, suffix) based on markers. If not found, body is entire content."""
    if MARKER_BEGIN in content and MARKER_END in content:
        pre, rest = content.split(MARKER_BEGIN, 1)
        body, post = rest.split(MARKER_END, 1)
        return pre, body, post
    return "", content, ""


def main() -> None:
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    generated: List[Path] = []
    for model_name, (csv_path, enum_name, _labels) in MODEL_SPECS.items():
        if not csv_path.exists():
            continue
        rows = read_csv_rows(csv_path)
        # Normalize column names for specific models
        if model_name == "Direction":
            for r in rows:
                if "direction_name" not in r and "direction" in r:
                    r["direction_name"] = r["direction"]
        body = build_enum_body(model_name, enum_name, rows)
        out_file = OUT_DIR / f"{enum_name.lower()}.py"

        # Determine existing body and version
        existing_version = 0
        existing_body = ""
        if out_file.exists():
            existing_content = out_file.read_text(encoding="utf-8")
            pre, body_prev, _post = split_existing_body(existing_content)
            existing_body = body_prev
            # Parse version line if present
            m = re.search(r"^# Version:\s*(\d+)\s*$", existing_content, re.MULTILINE)
            if m:
                try:
                    existing_version = int(m.group(1))
                except Exception:
                    existing_version = 0

        # Only write if body actually changed
        if body != existing_body:
            version = existing_version + 1
            now_iso = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
            header = (
                "# Auto-generated by scripts/autogen_models.py\n"
                "# Do not edit by hand\n"
                f"# Version: {version}\n"
                f"# Generated: {now_iso}\n"
                "from enum import Enum\n"
                f"from ..models import {model_name}\n\n"
                f"{MARKER_BEGIN}\n"
            )
            footer = f"{MARKER_END}\n"
            out_file.write_text(header + body + footer, encoding="utf-8")
            generated.append(out_file)
            print(f"Wrote {out_file} (v{version})")
        else:
            print(f"Unchanged: {out_file}")

    if not generated:
        print("No files generated (no CSVs present)")


if __name__ == "__main__":
    main()
